{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6714, 0.3286],\n",
      "        [0.7154, 0.2846]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 64)  # State and time as input\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_dim)\n",
    "\n",
    "    def forward(self, state_time):\n",
    "        stacked_tensor = torch.stack((state_time[0], state_time[1])).unsqueeze(0)\n",
    "        x = torch.relu(self.fc1(state_time))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return torch.softmax(x, dim=0)\n",
    "\n",
    "def get_action(state_time, policy_net):\n",
    "    with torch.no_grad():\n",
    "        action_probs = policy_net(torch.tensor(np.array(state_time),dtype=torch.float32)).detach().numpy()\n",
    "        action = np.random.choice(action_dim, p=action_probs.ravel())\n",
    "    return action\n",
    "\n",
    "def get_state(curr_state, drift, days, action):\n",
    "    \"\"\"\n",
    "    Calculate the next state based on the current state and drift.\n",
    "    \"\"\"\n",
    "    if action == 1:\n",
    "        return 2 * drift * days + 1\n",
    "    delta = np.random.randint(-drift, drift + 1, dtype=int)\n",
    "    return max(0, min(curr_state + delta, 2 * days * drift))\n",
    "\n",
    "def get_reward(curr_state, action, days, drift, start_price, strike_price):\n",
    "    \"\"\"\n",
    "    Calculate the reward based on the current state and action.\n",
    "    \"\"\"\n",
    "    if action == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return curr_state - days * drift + start_price - strike_price\n",
    "\n",
    "class TRPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, kl_constraint=0.01, max_kl_step=0.01, gamma=0.99, delta=0.01):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.kl_constraint = kl_constraint\n",
    "        self.max_kl_step = max_kl_step\n",
    "        self.gamma = gamma\n",
    "        self.delta = delta\n",
    "        \n",
    "        self.policy_net = PolicyNetwork(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.001)\n",
    "\n",
    "    def compute_advantage(self, rewards):\n",
    "        advantages = []\n",
    "        advantage = 0.0\n",
    "        for reward in reversed(rewards):\n",
    "            advantage = self.gamma * advantage + reward\n",
    "            advantages.insert(0, advantage)\n",
    "        return advantages\n",
    "\n",
    "    def surrogate_loss(self, old_probs, state_times, actions, advantages):\n",
    "        new_probs = self.policy_net(state_times)\n",
    "        old_probs = old_probs.gather(1, actions)\n",
    "        new_probs = new_probs.gather(1, actions)\n",
    "\n",
    "        ratio = new_probs / (old_probs + 1e-10)\n",
    "        surr_loss = torch.mean(ratio * advantages)\n",
    "        return surr_loss\n",
    "\n",
    "    def update_policy(self, samples):\n",
    "        state_times, actions, rewards, next_state_times = zip(*[(st, a, r, nst) for st, a, r, nst in samples])\n",
    "        rewards = np.array(rewards)\n",
    "        advantages = self.compute_advantage(rewards)\n",
    "        old_probs = torch.stack([self.policy_net(torch.tensor(np.array([state, time]),dtype=torch.float32)) for state, time in state_times])\n",
    "        print(old_probs)\n",
    "        # for _ in range(10):\n",
    "        #     policy_loss = -self.surrogate_loss(old_probs, \n",
    "        #                                         state_times, \n",
    "        #                                         torch.tensor(actions, dtype=torch.int64), \n",
    "        #                                         torch.tensor(advantages, dtype=torch.float32))\n",
    "        #     self.optimizer.zero_grad()\n",
    "        #     policy_loss.backward()\n",
    "        #     self.optimizer.step()\n",
    "\n",
    "        #     new_probs = torch.stack([self.policy_net(state, time) for state, time in state_times])\n",
    "\n",
    "\n",
    "\n",
    "        #     kl_div = torch.mean(new_probs * torch.log((new_probs + 1e-10) / (old_probs + 1e-10)))\n",
    "        #     if kl_div < self.kl_constraint:\n",
    "        #         break\n",
    "\n",
    "        # return kl_div.item()\n",
    "\n",
    "# Parameters for the environment\n",
    "days = 5\n",
    "drift = 5\n",
    "start_price = 500\n",
    "strike_price = 510\n",
    "\n",
    "# Parameters for TRPO\n",
    "state_dim = 2\n",
    "action_dim = 2\n",
    "num_episodes = 1000\n",
    "max_steps_per_episode = days\n",
    "\n",
    "agent = TRPOAgent(state_dim, action_dim)\n",
    "\n",
    "start_states = []\n",
    "for _ in range(1):  # Number of episodes\n",
    "    state = np.random.randint(0, 2 * days * drift + 1)\n",
    "    done = False\n",
    "    time = np.random.randint(0, days)\n",
    "    samples = []\n",
    "    start_states.append((state, time))\n",
    "    while not done:\n",
    "        action = get_action([state, time],agent.policy_net)\n",
    "        next_state = get_state(state, drift, days, action)\n",
    "        reward = get_reward(state, action, days, drift, start_price, strike_price)\n",
    "        next_time = time + 1\n",
    "\n",
    "        samples.append(((state, time), action, reward, (next_state, next_time)))\n",
    "\n",
    "        if time == days or action == 1:\n",
    "            done = True\n",
    "        state = next_state\n",
    "        time = next_time\n",
    "    agent.update_policy(samples)\n",
    "    # print(f\"Episode: {_}, KL Divergence: {kl_div}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
